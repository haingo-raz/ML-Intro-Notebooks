{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2 - Probability in Machine Learning\n",
    "\n",
    "Welcome to the Probability in Machine Learning Lab! In this lab, we will explore how probability theory plays a crucial role in machine learning. We will start with a simple coin flip example to grasp the basics and then move on to build a Bayesian email classifier. Let's dive in!\n",
    "\n",
    "## Setting Up the Environment\n",
    "\n",
    "First, let's import the necessary libraries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Coin Flip Probability Example\n",
    "\n",
    "### Objective:\n",
    "To understand basic probability and Python coding through a coin flip example.\n",
    "\n",
    "### Simulating Coin Flips\n",
    "We will simulate flipping a coin 1000 times.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tails' 'tails' 'heads' 'tails' 'heads' 'tails' 'tails' 'tails' 'heads'\n",
      " 'heads' 'tails' 'tails' 'heads' 'tails' 'heads' 'heads' 'tails' 'heads'\n",
      " 'heads' 'tails' 'tails' 'heads' 'tails' 'tails' 'tails' 'heads' 'heads'\n",
      " 'heads' 'tails' 'heads' 'tails' 'heads' 'tails' 'tails' 'tails' 'heads'\n",
      " 'heads' 'heads' 'heads' 'tails' 'heads' 'tails' 'heads' 'heads' 'heads'\n",
      " 'tails' 'tails' 'tails' 'tails' 'heads' 'tails' 'tails' 'heads' 'tails'\n",
      " 'tails' 'heads' 'heads' 'tails' 'tails' 'tails' 'tails' 'heads' 'tails'\n",
      " 'heads' 'tails' 'heads' 'tails' 'heads' 'heads' 'tails' 'heads' 'heads'\n",
      " 'heads' 'heads' 'heads' 'heads' 'heads' 'tails' 'tails' 'heads' 'heads'\n",
      " 'heads' 'tails' 'heads' 'tails' 'heads' 'heads' 'heads' 'tails' 'tails'\n",
      " 'tails' 'heads' 'tails' 'heads' 'heads' 'tails' 'heads' 'tails' 'heads'\n",
      " 'tails' 'tails' 'heads' 'heads' 'tails' 'tails' 'heads' 'heads' 'tails'\n",
      " 'tails' 'heads' 'tails' 'heads' 'tails' 'tails' 'tails' 'tails' 'heads'\n",
      " 'tails' 'tails' 'tails' 'heads' 'tails' 'tails' 'tails' 'heads' 'heads'\n",
      " 'tails' 'tails' 'tails' 'heads' 'tails' 'heads' 'tails' 'heads' 'tails'\n",
      " 'tails' 'tails' 'tails' 'tails' 'heads' 'heads' 'heads' 'tails' 'tails'\n",
      " 'heads' 'tails' 'heads' 'tails' 'heads' 'heads' 'heads' 'heads' 'heads'\n",
      " 'heads' 'tails' 'tails' 'heads' 'tails' 'tails' 'heads' 'tails' 'heads'\n",
      " 'heads' 'tails' 'heads' 'heads' 'tails' 'heads' 'tails' 'heads' 'tails'\n",
      " 'tails' 'tails' 'tails' 'heads' 'tails' 'tails' 'tails' 'tails' 'tails'\n",
      " 'tails' 'tails' 'tails' 'heads' 'tails' 'tails' 'tails' 'heads' 'tails'\n",
      " 'tails' 'tails' 'heads' 'heads' 'tails' 'heads' 'heads' 'tails' 'heads'\n",
      " 'heads' 'tails' 'heads' 'heads' 'tails' 'heads' 'tails' 'heads' 'heads'\n",
      " 'heads' 'heads' 'tails' 'heads' 'tails' 'heads' 'heads' 'tails' 'heads'\n",
      " 'heads' 'tails' 'heads' 'heads' 'heads' 'tails' 'heads' 'tails' 'tails'\n",
      " 'heads' 'heads' 'heads' 'heads' 'tails' 'tails' 'heads' 'tails' 'heads'\n",
      " 'tails' 'tails' 'tails' 'tails' 'tails' 'heads' 'heads' 'heads' 'heads'\n",
      " 'heads' 'tails' 'heads' 'tails' 'heads' 'tails' 'tails' 'tails' 'heads'\n",
      " 'heads' 'tails' 'heads' 'heads' 'tails' 'heads' 'heads' 'heads' 'heads'\n",
      " 'tails' 'tails' 'tails' 'tails' 'tails' 'heads' 'tails' 'heads' 'heads'\n",
      " 'heads' 'tails' 'heads' 'heads' 'heads' 'heads' 'tails' 'heads' 'tails'\n",
      " 'heads' 'heads' 'tails' 'tails' 'tails' 'tails' 'tails' 'heads' 'heads'\n",
      " 'tails' 'heads' 'tails' 'heads' 'heads' 'tails' 'tails' 'heads' 'heads'\n",
      " 'heads' 'tails' 'heads' 'heads' 'heads' 'tails' 'tails' 'heads' 'tails'\n",
      " 'tails' 'heads' 'heads' 'tails' 'tails' 'tails' 'heads' 'heads' 'heads'\n",
      " 'heads' 'tails' 'heads' 'tails' 'tails' 'tails' 'heads' 'heads' 'tails'\n",
      " 'heads' 'heads' 'heads' 'heads' 'heads' 'tails' 'tails' 'tails' 'heads'\n",
      " 'tails' 'heads' 'tails' 'heads' 'heads' 'heads' 'tails' 'heads' 'tails'\n",
      " 'heads' 'tails' 'heads' 'tails' 'heads' 'heads' 'tails' 'tails' 'tails'\n",
      " 'heads' 'tails' 'heads' 'heads' 'heads' 'heads' 'tails' 'tails' 'heads'\n",
      " 'tails' 'tails' 'tails' 'heads' 'tails' 'tails' 'tails' 'tails' 'tails'\n",
      " 'tails' 'heads' 'heads' 'tails' 'heads' 'heads' 'tails' 'heads' 'tails'\n",
      " 'tails' 'tails' 'heads' 'tails' 'tails' 'heads' 'tails' 'heads' 'tails'\n",
      " 'heads' 'tails' 'tails' 'heads' 'heads' 'tails' 'heads' 'tails' 'heads'\n",
      " 'tails' 'tails' 'tails' 'heads' 'heads' 'heads' 'tails' 'heads' 'tails'\n",
      " 'tails' 'tails' 'heads' 'tails' 'heads' 'heads' 'tails' 'heads' 'heads'\n",
      " 'heads' 'tails' 'heads' 'heads' 'tails' 'heads' 'tails' 'tails' 'tails'\n",
      " 'tails' 'tails' 'heads' 'tails' 'tails' 'heads' 'heads' 'tails' 'tails'\n",
      " 'tails' 'tails' 'tails' 'tails' 'heads' 'tails' 'heads' 'tails' 'tails'\n",
      " 'heads' 'tails' 'tails' 'tails' 'tails' 'tails' 'heads' 'tails' 'heads'\n",
      " 'tails' 'heads' 'tails' 'heads' 'heads' 'heads' 'tails' 'tails' 'tails'\n",
      " 'heads' 'tails' 'heads' 'heads' 'heads' 'heads' 'heads' 'heads' 'tails'\n",
      " 'tails' 'tails' 'heads' 'heads' 'tails' 'heads' 'tails' 'tails' 'tails'\n",
      " 'tails' 'heads' 'heads' 'heads' 'heads' 'heads' 'heads' 'tails' 'tails'\n",
      " 'heads' 'tails' 'heads' 'heads' 'heads' 'heads' 'heads' 'tails' 'heads'\n",
      " 'tails' 'heads' 'tails' 'heads' 'tails' 'tails' 'heads' 'heads' 'tails'\n",
      " 'tails' 'heads' 'tails' 'heads' 'tails' 'heads' 'tails' 'tails' 'heads'\n",
      " 'heads' 'tails' 'tails' 'heads' 'tails' 'tails' 'heads' 'tails' 'heads'\n",
      " 'tails' 'heads' 'heads' 'tails' 'tails' 'tails' 'tails' 'tails' 'tails'\n",
      " 'tails' 'tails' 'heads' 'heads' 'tails' 'heads' 'tails' 'heads' 'heads'\n",
      " 'heads' 'heads' 'tails' 'tails' 'heads' 'tails' 'heads' 'tails' 'heads'\n",
      " 'tails' 'heads' 'tails' 'heads' 'tails' 'tails' 'tails' 'tails' 'heads'\n",
      " 'tails' 'tails' 'tails' 'tails' 'heads' 'heads' 'tails' 'tails' 'heads'\n",
      " 'tails' 'tails' 'heads' 'heads' 'heads' 'heads' 'heads' 'tails' 'tails'\n",
      " 'heads' 'heads' 'heads' 'heads' 'tails' 'tails' 'heads' 'heads' 'heads'\n",
      " 'tails' 'heads' 'tails' 'tails' 'tails' 'heads' 'tails' 'heads' 'tails'\n",
      " 'tails' 'heads' 'tails' 'heads' 'heads' 'heads' 'tails' 'tails' 'heads'\n",
      " 'tails' 'heads' 'tails' 'heads' 'heads' 'heads' 'heads' 'tails' 'heads'\n",
      " 'heads' 'tails' 'tails' 'heads' 'heads' 'heads' 'tails' 'tails' 'heads'\n",
      " 'heads' 'tails' 'heads' 'tails' 'tails' 'heads' 'tails' 'heads' 'tails'\n",
      " 'tails' 'tails' 'heads' 'tails' 'heads' 'tails' 'heads' 'heads' 'tails'\n",
      " 'tails' 'tails' 'heads' 'heads' 'heads' 'tails' 'heads' 'tails' 'heads'\n",
      " 'tails' 'tails' 'tails' 'heads' 'tails' 'tails' 'heads' 'heads' 'heads'\n",
      " 'tails' 'tails' 'tails' 'heads' 'tails' 'heads' 'heads' 'tails' 'tails'\n",
      " 'heads' 'tails' 'tails' 'heads' 'heads' 'heads' 'tails' 'heads' 'heads'\n",
      " 'tails' 'heads' 'tails' 'heads' 'tails' 'tails' 'tails' 'tails' 'heads'\n",
      " 'tails' 'tails' 'heads' 'tails' 'heads' 'heads' 'heads' 'heads' 'heads'\n",
      " 'heads' 'heads' 'tails' 'tails' 'heads' 'heads' 'heads' 'heads' 'heads'\n",
      " 'heads' 'tails' 'tails' 'tails' 'tails' 'tails' 'tails' 'tails' 'tails'\n",
      " 'heads' 'tails' 'heads' 'tails' 'tails' 'tails' 'heads' 'tails' 'tails'\n",
      " 'heads' 'heads' 'tails' 'tails' 'tails' 'tails' 'tails' 'tails' 'heads'\n",
      " 'heads' 'heads' 'tails' 'tails' 'tails' 'tails' 'tails' 'heads' 'heads'\n",
      " 'heads' 'heads' 'tails' 'tails' 'heads' 'tails' 'heads' 'heads' 'heads'\n",
      " 'tails' 'tails' 'tails' 'tails' 'heads' 'tails' 'tails' 'tails' 'tails'\n",
      " 'tails' 'tails' 'heads' 'heads' 'heads' 'tails' 'heads' 'tails' 'heads'\n",
      " 'tails' 'heads' 'tails' 'heads' 'heads' 'heads' 'tails' 'heads' 'tails'\n",
      " 'tails' 'tails' 'heads' 'heads' 'heads' 'tails' 'heads' 'tails' 'tails'\n",
      " 'heads' 'tails' 'heads' 'heads' 'heads' 'tails' 'tails' 'tails' 'tails'\n",
      " 'tails' 'heads' 'heads' 'heads' 'heads' 'tails' 'heads' 'heads' 'tails'\n",
      " 'tails' 'tails' 'heads' 'tails' 'tails' 'tails' 'tails' 'tails' 'tails'\n",
      " 'tails' 'tails' 'tails' 'heads' 'tails' 'tails' 'tails' 'tails' 'heads'\n",
      " 'heads' 'heads' 'heads' 'heads' 'tails' 'tails' 'heads' 'heads' 'heads'\n",
      " 'tails' 'tails' 'heads' 'tails' 'tails' 'heads' 'tails' 'tails' 'heads'\n",
      " 'heads' 'heads' 'tails' 'heads' 'heads' 'heads' 'heads' 'tails' 'tails'\n",
      " 'heads' 'heads' 'heads' 'tails' 'tails' 'heads' 'tails' 'tails' 'heads'\n",
      " 'heads' 'tails' 'tails' 'heads' 'tails' 'heads' 'heads' 'tails' 'tails'\n",
      " 'heads' 'heads' 'tails' 'tails' 'heads' 'heads' 'heads' 'heads' 'tails'\n",
      " 'tails' 'tails' 'tails' 'heads' 'heads' 'heads' 'heads' 'tails' 'tails'\n",
      " 'heads' 'tails' 'heads' 'tails' 'heads' 'heads' 'tails' 'tails' 'heads'\n",
      " 'heads' 'tails' 'heads' 'tails' 'heads' 'heads' 'tails' 'tails' 'tails'\n",
      " 'heads' 'tails' 'tails' 'tails' 'tails' 'heads' 'heads' 'tails' 'heads'\n",
      " 'tails' 'heads' 'heads' 'tails' 'heads' 'heads' 'tails' 'tails' 'tails'\n",
      " 'heads' 'tails' 'tails' 'tails' 'heads' 'tails' 'tails' 'tails' 'heads'\n",
      " 'heads' 'heads' 'tails' 'heads' 'tails' 'tails' 'heads' 'heads' 'heads'\n",
      " 'tails' 'tails' 'heads' 'heads' 'heads' 'tails' 'heads' 'tails' 'tails'\n",
      " 'tails' 'heads' 'heads' 'heads' 'heads' 'heads' 'tails' 'tails' 'heads'\n",
      " 'tails' 'tails' 'heads' 'heads' 'tails' 'tails' 'tails' 'heads' 'heads'\n",
      " 'heads' 'heads' 'tails' 'tails' 'tails' 'tails' 'tails' 'heads' 'heads'\n",
      " 'tails' 'tails' 'heads' 'tails' 'tails' 'tails' 'heads' 'tails' 'heads'\n",
      " 'heads' 'tails' 'heads' 'tails' 'heads' 'heads' 'heads' 'tails' 'heads'\n",
      " 'heads' 'tails' 'heads' 'heads' 'heads' 'heads' 'heads' 'heads' 'heads'\n",
      " 'heads']\n",
      "    flip_result\n",
      "0         tails\n",
      "1         tails\n",
      "2         heads\n",
      "3         tails\n",
      "4         heads\n",
      "..          ...\n",
      "995       heads\n",
      "996       heads\n",
      "997       heads\n",
      "998       heads\n",
      "999       heads\n",
      "\n",
      "[1000 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# Simulating 1000 coin flips, 0 for 'tails' and 1 for 'heads'\n",
    "coin_flips = np.random.choice(['heads', 'tails'], size=1000)\n",
    "df_coin = pd.DataFrame({'flip_result': coin_flips})\n",
    "print(coin_flips)\n",
    "print(df_coin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Flip Results\n",
    "Now, let's count how many heads and tails we got."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tails    511\n",
      "heads    489\n",
      "Name: flip_result, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "flip_counts = df_coin['flip_result'].value_counts()\n",
    "print(flip_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Probabilities\n",
    "Next, we will calculate the probability of getting heads or tails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of Heads: 0.502\n",
      "Probability of Tails: 0.498\n"
     ]
    }
   ],
   "source": [
    "p_heads = flip_counts['heads'] / len(df_coin)\n",
    "p_tails = flip_counts['tails'] / len(df_coin)\n",
    "print(f\"Probability of Heads: {p_heads}\")\n",
    "print(f\"Probability of Tails: {p_tails}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Bayesian Email Classifier\n",
    "\n",
    "### Objective:\n",
    "Now, you will build a Bayesian email classifier to differentiate between 'spam' and 'ham' (not spam) emails.\n",
    "\n",
    "### Task 1: Exploring the Dataset\n",
    "First, load and explore the dataset. You can either find and use a dataset or use the following code to simulate a sample dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     email_length  contains_free  contains_winner time_of_day label\n",
      "0             109              0                0     morning   ham\n",
      "1              97              0                0     morning  spam\n",
      "2             112              0                0     morning  spam\n",
      "3             130              1                0   afternoon   ham\n",
      "4              95              0                1   afternoon  spam\n",
      "..            ...            ...              ...         ...   ...\n",
      "995            94              0                1       night   ham\n",
      "996           135              0                0       night  spam\n",
      "997           112              0                0     evening  spam\n",
      "998            88              0                1   afternoon  spam\n",
      "999           111              0                0     evening  spam\n",
      "\n",
      "[1000 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# The following code snippet creates a simulated email classification (spam and not spam) dataset \n",
    "# with 1000 data points.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Sample size\n",
    "n_samples = 1000\n",
    "\n",
    "# Simulating data\n",
    "np.random.seed(42)\n",
    "data = {\n",
    "    'email_length': np.random.normal(100, 20, n_samples).astype(int),\n",
    "    'contains_free': np.random.choice([0, 1], size=n_samples, p=[0.7, 0.3]),\n",
    "    'contains_winner': np.random.choice([0, 1], size=n_samples, p=[0.8, 0.2]),\n",
    "    'time_of_day': np.random.choice(['morning', 'afternoon', 'evening', 'night'], n_samples),\n",
    "    'label': np.random.choice(['spam', 'ham'], n_samples, p=[0.4, 0.6])\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df)\n",
    "\n",
    "# Saving the dataset\n",
    "df.to_csv('simulated_email_dataset.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>email_length</th>\n",
       "      <th>contains_free</th>\n",
       "      <th>contains_winner</th>\n",
       "      <th>time_of_day</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>109</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>morning</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>97</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>morning</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>112</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>morning</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>130</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>afternoon</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>95</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>afternoon</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   email_length  contains_free  contains_winner time_of_day label\n",
       "0           109              0                0     morning   ham\n",
       "1            97              0                0     morning  spam\n",
       "2           112              0                0     morning  spam\n",
       "3           130              1                0   afternoon   ham\n",
       "4            95              0                1   afternoon  spam"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset (Replace 'path_to_dataset' with the actual file path). You can uncomment the codes below. Notice what `df_emails.head()` is representing.\n",
    "df_emails = pd.read_csv('simulated_email_dataset.csv')\n",
    "\n",
    "# This command prints the first 5 rows of the dataset\n",
    "df_emails.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Data Preprocessing\n",
    "You need to preprocess the data for analysis. This involves normalizing and encoding the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check for missing values:\n",
      "(array([], dtype=int64), array([], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "# Your code for Data Preprocessing goes here\n",
    "\n",
    "# Checking for missing values in the df_emails dataframe\n",
    "print(\"Check for missing values:\")\n",
    "missing_values = np.where(pd.isnull(df_emails))\n",
    "print(missing_values)\n",
    "# The command returns an empty array which means there are no empty values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for duplicated rows:\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Checking for duplicated rows\n",
    "print(\"Checking for duplicated rows:\")\n",
    "duplicated_rows = df_emails.duplicated().any()\n",
    "print(duplicated_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# Droping duplicated rows\n",
    "df_no_duplicates = df_emails.drop_duplicates()\n",
    "\n",
    "print(df_no_duplicates.duplicated().any())\n",
    "# df_no_duplicates does not have any duplicated values anymore as the above command returns false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the data types\n",
      "email_length        int64\n",
      "contains_free       int64\n",
      "contains_winner     int64\n",
      "time_of_day        object\n",
      "label              object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Checking the data types\n",
    "print(\"Checking the data types\")\n",
    "print(df_no_duplicates.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows and Columns left for unique rows\n",
      "   email_length  contains_free  contains_winner time_of_day label  \\\n",
      "0           109              0                0     morning   ham   \n",
      "1            97              0                0     morning  spam   \n",
      "2           112              0                0     morning  spam   \n",
      "3           130              1                0   afternoon   ham   \n",
      "4            95              0                1   afternoon  spam   \n",
      "\n",
      "   time_of_day_encoded  \n",
      "0                    0  \n",
      "1                    0  \n",
      "2                    0  \n",
      "3                    1  \n",
      "4                    1  \n",
      "(724, 6)\n"
     ]
    }
   ],
   "source": [
    "# Converting categorical data to numerical data in time_of_day using label encoding\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "df_no_duplicates_copy = df_no_duplicates.copy()\n",
    "\n",
    "# Define a mapping for label encoding\n",
    "time_of_day_mapping = {'morning': 0, 'afternoon': 1, 'evening': 2, 'night': 3}\n",
    "\n",
    "# Apply label encoding using map\n",
    "df_no_duplicates_copy.loc[:, 'time_of_day_encoded'] = df_no_duplicates_copy['time_of_day'].map(time_of_day_mapping)\n",
    "\n",
    "df_no_duplicates_copy.to_csv('cleaned_dataset.csv')\n",
    "print(\"Rows and Columns left for unique rows\")\n",
    "\n",
    "print(df_no_duplicates_copy.head())\n",
    "print(df_no_duplicates_copy.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Probability Calculation\n",
    "Calculate the probability of spam and ham emails in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of ham: 0.5676795580110497\n",
      "Probability of spam: 0.43232044198895025\n"
     ]
    }
   ],
   "source": [
    "# Your code for calculating the probability of spam and ham emails in the dataset goes here\n",
    "\n",
    "# We will be using the cleaned the dataset\n",
    "df_cleaned = pd.read_csv('cleaned_dataset.csv')\n",
    "\n",
    "email_counts = df_cleaned['label'].value_counts()\n",
    "\n",
    "p_spam = email_counts['spam'] / len(df_cleaned)\n",
    "p_ham = email_counts['ham'] / len(df_cleaned)\n",
    "print(f\"Probability of ham: {p_ham}\")\n",
    "print(f\"Probability of spam: {p_spam}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Implementing Bayes' Theorem\n",
    "Implement Bayes' Theorem to classify emails as spam or ham."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function using Bayes' Theorem for classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5: Model Testing\n",
    "Test the model on a new dataset and evaluate its performance. You can use a subset of the dataset that you created or create a new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6: Discussion\n",
    "1. Which probability distribution would you choose for an email classifier? Explain your answer.\n",
    "2. Discuss how Bayesian updating improves the accuracy of the classifier.\n",
    "3. What are the limitations of the model built in this lab?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1. For an email classifier, I would choose the Multinomial Naive Bayes probability distribution. It is based on Bayes' theorem and can be used for text classification. The multinomial Naive Bayes also allows to analyse the frequency of words which might be a plus in terms of detecting spams.\n",
    "\n",
    "GeeksforGeeks (2024) Multinomial Naive Bayes. Available at: https://www.geeksforgeeks.org/multinomial-naive-bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2.Bayesian updating improves the accuracy of the classifier with the help of new data that is coming in by continuously updating the probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3. The model built in this label was built based on randomly generated data.\n",
    "While Naive Bayes assumes that all the features used are independent, in this lab some of the features might be correlated (example: contains_free and contains_winner)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "Submit a link to your completed Jupyter Notebook file hosted on your private GitHub repository through the submission link in Blackboard."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
