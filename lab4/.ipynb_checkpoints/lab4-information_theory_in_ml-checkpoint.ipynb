{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4 - Information Theory in Machine Learning\n",
    "\n",
    "Welcome to this week's lab on Information Theory! This week, we will dive into the fascinating world of Information Theory as applied to Machine Learning. Specifically, we will focus on two key concepts: Entropy and Information Gain. These principles are fundamental in understanding how decision trees make split decisions to organize data effectively.\n",
    "\n",
    "### Entropy\n",
    "- Entropy, in the context of information theory, measures the level of uncertainty or disorder within a set of data.\n",
    "- In machine learning, particularly in decision trees, entropy helps to determine how a dataset should be split. A high entropy means more disorder, indicating that our dataset is varied. Conversely, low entropy suggests more uniformity in the data.\n",
    "\n",
    "### Information Gain\n",
    "- Information Gain measures the reduction in entropy after the dataset is split on an attribute.\n",
    "- It is crucial in building decision trees as it helps to decide the order of attributes the tree will use for splitting the data. The attribute with the highest Information Gain is chosen as the splitting attribute at each node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Entropy and Information Gain in Decision Trees\n",
    "Decision Trees use these concepts to create branches. By choosing splits that maximize Information Gain (or equivalently minimize entropy), a decision tree can effectively categorize data, leading to better classification or regression models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load and Explore the Iris Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0                  5.1               3.5                1.4               0.2   \n",
       "1                  4.9               3.0                1.4               0.2   \n",
       "2                  4.7               3.2                1.3               0.2   \n",
       "3                  4.6               3.1                1.5               0.2   \n",
       "4                  5.0               3.6                1.4               0.2   \n",
       "..                 ...               ...                ...               ...   \n",
       "145                6.7               3.0                5.2               2.3   \n",
       "146                6.3               2.5                5.0               1.9   \n",
       "147                6.5               3.0                5.2               2.0   \n",
       "148                6.2               3.4                5.4               2.3   \n",
       "149                5.9               3.0                5.1               1.8   \n",
       "\n",
       "     target  \n",
       "0         0  \n",
       "1         0  \n",
       "2         0  \n",
       "3         0  \n",
       "4         0  \n",
       "..      ...  \n",
       "145       2  \n",
       "146       2  \n",
       "147       2  \n",
       "148       2  \n",
       "149       2  \n",
       "\n",
       "[150 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "df['target'] = iris.target\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Calculate Entropy\n",
    "To calculate the `entropy` we need to:\n",
    "- First, extract the target variable `y` from your dataset (like the 'target' column in the Iris dataset).\n",
    "- Then, call `calculate_entropy(y)` to get the entropy.\n",
    "\n",
    "This function calculates the entropy of a given target variable `y`. It works by first determining the unique classes in `y`, then computes the probability of each class, and uses this probability to calculate the entropy. This is a crucial step in understanding the disorder or uncertainty in the dataset, a fundamental concept in information theory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.584962500721156"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_entropy(y):\n",
    "    class_labels = np.unique(y)\n",
    "    entropy = 0\n",
    "    for label in class_labels:\n",
    "        probability = len(y[y == label]) / len(y)\n",
    "        entropy -= probability * np.log2(probability)\n",
    "    return entropy\n",
    "\n",
    "calculate_entropy(df['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is your observastion about the calculated Entropy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The calculated entropy returns *1.584962500721156*. It represents the bit of uncertainty when predicting the species of an Iris flower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Calculate Information Gain\n",
    "There are three steps for calculating the Information Gain:\n",
    "1. Compute Overall Entropy: Use the entropy function from Step 3 on the entire target dataset.\n",
    "2. Calculate Weighted Entropy for Each Attribute: For each unique value in the attribute, partition the dataset and calculate its entropy. Then calculate the weighted sum of these entropies, where the weights are the proportions of instances in each partition.\n",
    "3. Compute Information Gain: Subtract the weighted entropy of the split from the original entropy.\n",
    "\n",
    "The attribute with the highest Information Gain is generally chosen for splitting, as it provides the most significant reduction in uncertainty. This step is critical in constructing an effective decision tree, as it directly influences the structure and depth of the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8769376208910578\n",
      "0.5166428756804977\n",
      "1.4463165236458\n",
      "1.4358978386754417\n"
     ]
    }
   ],
   "source": [
    "def calculate_information_gain(df, attribute, target_name):\n",
    "    total_entropy = calculate_entropy(df[target_name])\n",
    "    values, counts = np.unique(df[attribute], return_counts=True)\n",
    "    weighted_entropy = sum((counts[i] / sum(counts)) * calculate_entropy(df.where(df[attribute] == values[i]).dropna()[target_name]) for i in range(len(values)))\n",
    "    information_gain = total_entropy - weighted_entropy\n",
    "    return information_gain\n",
    "\n",
    "sepal_length_ig = calculate_information_gain(df, 'sepal length (cm)', 'target')\n",
    "print(sepal_length_ig)\n",
    "\n",
    "sepal_width_ig = calculate_information_gain(df, 'sepal width (cm)', 'target')\n",
    "print(sepal_width_ig)\n",
    "\n",
    "petal_length_ig = calculate_information_gain(df, 'petal length (cm)', 'target')\n",
    "print(petal_length_ig)\n",
    "\n",
    "petal_width_ig = calculate_information_gain(df, 'petal width (cm)', 'target')\n",
    "print(petal_width_ig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discuss your findings here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above results, the attribute with the highest information gain is `petal length (cm)` with 1.4463165236458 information gain. Therefore, this attribute should be the first one to be splitted in order to provide the purest subsets. It is followed by `petal width (cm)` (1.4358978386754417), `sepal length (cm)`(0.8769376208910578) and `sepal width (cm)`(0.5166428756804977)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Apply Entropy and Information Gain on a different dataset\n",
    "\n",
    "Your task is to choose a new dataset and implement what you learned in `Part 1` on this new dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Implement Entropy and Information Gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash', 'magnesium',\n",
       "       'total_phenols', 'flavanoids', 'nonflavanoid_phenols',\n",
       "       'proanthocyanins', 'color_intensity', 'hue',\n",
       "       'od280/od315_of_diluted_wines', 'proline', 'target'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Selecting the wine dataset from sklearn\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "wine = load_wine()\n",
    "df_wine = pd.DataFrame(wine.data, columns=wine.feature_names)\n",
    "\n",
    "# Set the target variable\n",
    "df_wine['target'] = wine.target\n",
    "\n",
    "df_wine.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5668222768551812"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating the entropy\n",
    "calculate_entropy(df_wine['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The entropy returns 1.5668222768551812 in the wine dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alcohol: 1.3406163077175945\n",
      "malic_acid: 1.2842014538975586\n",
      "ash: 0.7863398170147985\n",
      "alcalinity_of_ash: 0.7357938479915789\n",
      "magnesium: 0.6361513778259005\n",
      "total_phenols: 1.1812650269501774\n",
      "flavanoids, 1.4207548611248442\n",
      "nonflavanoid_phenols:0.4739066456615457\n",
      "proanthocyanins:1.1161486280647779\n",
      "color_intensity:1.3700832318627436\n",
      "hue:1.0413965047465832\n",
      "od280/od315_of_diluted_wines:1.3470502797834747\n",
      "proline:1.3015454175918462\n"
     ]
    }
   ],
   "source": [
    "# Calculating information gains for each other attributes\n",
    "alcohol_ig = calculate_information_gain(df_wine, 'alcohol', 'target')\n",
    "print(f\"alcohol: {alcohol_ig}\")\n",
    "\n",
    "malic_acid_ig = calculate_information_gain(df_wine, 'malic_acid', 'target')\n",
    "print(f\"malic_acid: {malic_acid_ig}\")\n",
    "\n",
    "ash_ig = calculate_information_gain(df_wine, 'ash', 'target')\n",
    "print(f\"ash: {ash_ig}\")\n",
    "\n",
    "alcalinity_of_ash_ig = calculate_information_gain(df_wine, 'alcalinity_of_ash', 'target')\n",
    "print(f\"alcalinity_of_ash: {alcalinity_of_ash_ig}\")\n",
    "\n",
    "magnesium_ig = calculate_information_gain(df_wine, 'magnesium', 'target')\n",
    "print(f\"magnesium: {magnesium_ig}\")\n",
    "\n",
    "total_phenols_ig = calculate_information_gain(df_wine, 'total_phenols', 'target')\n",
    "print(f\"total_phenols: {total_phenols_ig}\")\n",
    "\n",
    "flavanoids_ig = calculate_information_gain(df_wine, 'flavanoids', 'target')\n",
    "print(f\"flavanoids, {flavanoids_ig}\")\n",
    "\n",
    "nonflavanoid_phenols_ig = calculate_information_gain(df_wine, 'nonflavanoid_phenols', 'target')\n",
    "print(f\"nonflavanoid_phenols:{ nonflavanoid_phenols_ig}\")\n",
    "\n",
    "proanthocyanins_ig = calculate_information_gain(df_wine, 'proanthocyanins', 'target')\n",
    "print(f\"proanthocyanins:{proanthocyanins_ig}\")\n",
    "\n",
    "color_intensity_ig = calculate_information_gain(df_wine, 'color_intensity', 'target')\n",
    "print(f\"color_intensity:{color_intensity_ig}\")\n",
    "\n",
    "hue_ig = calculate_information_gain(df_wine, 'hue', 'target')\n",
    "print(f\"hue:{hue_ig}\")\n",
    "\n",
    "od280_od315_of_diluted_wines_ig = calculate_information_gain(df_wine, 'od280/od315_of_diluted_wines', 'target')\n",
    "print(f\"od280/od315_of_diluted_wines:{od280_od315_of_diluted_wines_ig}\")\n",
    "\n",
    "proline_ig = calculate_information_gain(df_wine, 'proline', 'target')\n",
    "print(f\"proline:{proline_ig}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Discuss your findings in detail\n",
    "Provide detailed explanation and discussion about your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attribute `flavanoids` (1.4207548611248442) returns the highest information gain in the wine dataset. Which means that it should be the fist node to be splitted to get the purest subset. \n",
    "\n",
    "The following nodes with the highest information gain are:\n",
    "- color_intensity ( 1.3700832318627436), \n",
    "- od280/od315_of_diluted_wines (1.3470502797834747), \n",
    "- alcohol (1.3406163077175945), \n",
    "- proline (1.3015454175918462), \n",
    "- total_phenols (1.1812650269501774), \n",
    "- malic_acid (1.2842014538975586), \n",
    "- proanthocyanins (1.1161486280647779), \n",
    "- hue (1.0413965047465832), \n",
    "- alcalinity_of_ash (0.7357938479915789), \n",
    "- ash (0.7863398170147985), \n",
    "- magnesium (0.6361513778259005), \n",
    "- nonflavanoid_phenols (0.4739066456615457)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "Submit a link to your completed Jupyter Notebook file hosted on your private GitHub repository through the submission link in Blackboard."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
